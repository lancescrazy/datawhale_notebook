# 提升kaggle模型的实用小技巧

##　一、经常回国过去的比赛（温故知新）

比赛中经常会出现非常相似问题的翻版

> 某些主题每年都会提出一个常规挑战，而只有一些很小的挑战
>
> 某些领域比赛目标不同，但本质一样

回顾获奖者的解决方案，某些流行的基准模型总是做的很好

> 卷积神经网络或 cv 挑战中更复杂的 ResNet 或 EfficientNet
>
> WageNet 在音频处理中，若只是用Mel Spectrogram，也可以用图像识别模型处理
>
> BERT 及其衍生产品（RoBERTa）在自然语言处理中的挑战
>
> Light Gradient Boosting (或其他梯度提升，或者树策略)对表格数据的处理
>
> 超参数的选择，额外的工具，专注，或者整合公共内核

## 二、花足够的时间在数据预处理上

> 1. 清理数据：填充NaNs，去除离群值，将数据分成同质观测值的类别
> 2. 做一些简单的探索性数据分析
> 3. **增强数据**：（也不要让数据太大，无法处理）互联网额外数据集，kaggle上可以使用的，对已给数据进行：翻转，剪裁图像，叠加音频记录，反向翻译，替换文本中的同义词
> 4. 交叉验证方法：Trust Your CV，如何分割数据，一个聪明的CV策略

## 三、尝试超参数搜索

无需手工实验，找到最佳参数，最常见的策略有：

> 1. 网格搜索：千万不要：某些之，会完全错过某个模式或性能的局部峰值，
> 2. 随机搜索（及其Monte-Carlo衍生物）：尝试随机参数，并行方法，成本高，有点事，可以加入先验知识，如你支持某个值a，就可以从以a为中心的对数正态分布中抽取样本
> 3. 贝叶斯搜搜：随机搜索--》改进后，成为低成本迭代方法，根据当前模型迭代评估一个有希望的超参数配置，并更新，效果最好的一种
> 4. 其他：基于梯度搜索、演化优化，比较危险

## 四、简单的包装器改变游戏规则

模型包装器，在不同级别上工作

> 1. 优化过程中，添加一个学习率调度器，获得更准确的训练
> 2. 优化过程中，Lookahead包在优化器上，：向前走k个优化步骤，得到性能最好的，然后向最佳方向后退一步，开始训练，主要可以稳定训练（数据嘈杂时）
> 3. 训练前，给权重一个合适的初始化，对流行架构--基准权重，否则--Layer Sequential Unit Variance初始化（LSUV），初始化为蒸饺权重，且在可训练层中时单位方差
> 4. 从NN最后一层训练LGBM，而不是softmax输出层，效果会更好

## 五、bagging集成在一起

总是保存运行的每一个模型预测，然后对所有模型平均（是否成立：聪明的集成，按模型的单一性能加权，在最终得分中增加了什么）

对公共内核装袋，集成策略中模型越多，越可能获胜

[reference](https://towardsdatascience.com/5-simple-tips-to-improve-your-kaggle-models-159c00523418)





























































